## ch8-sec1
T1
（1）对于正弦函数预测，观测结果包含一个周期直观上合理一些。
（2）一个，无噪音，那么根据上一个观测然后带入导数公式预测

T2
历史数据的分布和未来数据的分布不是一回事，而且无法预测未来某个突发性事件对证券价格的冲击。

T3
文本类型的预测？ 但是文本的词性判别、一词多义、填空，需要结合上下文来看；

T4
文本分析？求解隐马尔可夫模型；

## ch8-sec2
T1
词元化方法：gensim、nltk，Keras，Torchtext；中文有jieba分词库。
粗糙做法，根据空格分隔；精细化做法，正则表达式分割；更进一步在tokenize过程中，要考虑到一些特殊的词语的转换:"can't -> can not"、"don't -> do not"，以及不同地域的英语一些特殊表达的近似"wanna -> want to"、"gonna -> go to"。

T2
`min_freq`参数越高，那么词汇量就会越小；`tokenize`转化为单词或字符，没影响吧，前者针对英文，后者针对中文汉字，当然后者如果使用英文，会返回26个英文字母。

## ch8-sec3
T1
100000-4+1=99997

T2
猜测是依照序列记录说话者身份以及说话内容以及其中的顺序。构成一个序列中的序列数据。

T3
x，y的变量，取对数，然后计算其斜率。可以粗略两点法计算；也可以利用回归。

T4
长序列数据，分批读，如果很大很大，大数据分析，那就hadoop，spark等大数据分布式处理工具。

T5
(1)随机抽取，引入多样的信息，防止过拟合
(2)几乎是uniform分布，但是直观感觉前几个和后几个语句，抽到的概率小一些
(3)直接根据uniform分布提取序列。

T6 
小批量，那么很多长句被截短；可以考虑取最大长然后padding。

## ch8-sec4
T1
输出维度是unique词元数量
T2
因为隐藏层H，通过timestep维度的循环迭代，逐次包含了先前词元的信息。
T3
长序列学习，相当于有很多个系数矩阵相乘，导致了梯度爆炸和梯度消失的问题加剧。模型性能下降。
T4
语言预测，文本生成，词性分析。

## ch8-sec5
T1
独热编码可以保证不同的字母表示不同，而每个单词由字母组成，就确保了，不同单词字母序列的嵌入表示不同。

T2
调参题

T3
code

T4
梯度爆炸，训练RNN的常见问题

T5
改变顺序，困惑度升高？

T6
ReLU激活函数的梯度，小于等于1，理论上而言，如果weight很大，就会一直梯度爆炸，所以官方RNN API默认激活函数都是tanh。但是实际编码中，发现换成relu，不会出事，原因可能和pytorch将权重系数w初始化为单位阵I有关。

## ch8-sec6
T1
增加训练轮数，减小样本数量，选取大的learning_rate使之过拟合；

T2
运行变慢，梯度爆炸；在optimizer优化的过程中，使用梯度裁剪

T3
code

## ch8-sec7
T1
(1)
Proof
$$
\begin{equation*}
\begin{split} 
&M = V^{-1}diag\{\lambda_1, \cdots,\lambda_n \}V \\
&M^k = V^{-1} diag\{\lambda_1^k,\cdots,\lambda_n^k \}V
\end{split}
\end{equation*} 
$$ 如果M不能对角化，那么中间的对角阵转化成Jordan块也能得出相同结论；
(2)
假设x在Rn上服从均匀分布，也就是说，选到$v_1，\cdots, v_n$的概率等价，那么$M^kv_i = \lambda_i^k v_i$可得，对于$v_1$,$M^kx$与之共线的概率$p \sim \frac{\lambda_1^k}{\sum\limits_{i}^n \lambda_i^k}$ 相较于其他特征向量来说最大。（naive想法应该是把x看成$v_1，\cdots, v_n$基向量的线性组合，由于和$M^k$相乘得到的$\lambda_1^k$占主要成分，导致了得到的结果很接近$v_1$所以会有命题的说法。）
(3)
循环神经网络层数越深，梯度爆炸的程度越严重，且发生梯度爆炸的概率越大
T2
梯度裁剪；更改网络结构，根据数据量，选择合适的深度；权重正则化，L2惩罚会以一个固定比率缩减权重；权值合理初始化；Batch normalization；
