{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_inline import backend_inline\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用到的部分d2l code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_svg_display():\n",
    "    backend_inline.set_matplotlib_formats('svg')\n",
    "\n",
    "\n",
    "def set_figsize(figsize=(3.5, 2.5)):\n",
    "    use_svg_display()\n",
    "    plt.rcParams['figure.figsize'] = figsize\n",
    "\n",
    "\n",
    "def synthetic_data(w, b, num_examples):  #@save\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))#生成均值为0，方差为1，数据纬度是（num_examples, len(w)）的随机数据作为训练样本\n",
    "    y = torch.matmul(X, w) + b #生成X对应的预测值y\n",
    "    y += torch.normal(0, 0.01, y.shape)# 加入噪音，加入的是均值为0，方差为0.01，纬度和y.shape一致的噪音进行干扰\n",
    "    return X, y.reshape((-1, 1))#返回X，y，y为列向量\n",
    "\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features) #获取样本大小\n",
    "    indices = list(range(num_examples)) #获取样本脚标的list\n",
    "    # 这些样本是随机读取的，没有特定的顺序\n",
    "    random.shuffle(indices) #随机变换indices\n",
    "    for i in range(0, num_examples, batch_size): #开始循环\n",
    "        batch_indices = torch.tensor(indices[i: min(i + batch_size, num_examples)]) #有可能不能整除，取i + batch_size和num_examples的较小值\n",
    "        yield features[batch_indices], labels[batch_indices] #相当于是一个迭代器，每次返回batch_size个样本\n",
    "\n",
    "def linreg(X, w, b):  #@save\n",
    "    return torch.matmul(X, w) + b\n",
    "\n",
    "def squared_loss(y_hat, y):  #@save\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n",
    "\n",
    "def sgd(params, lr, batch_size):  #@save\n",
    "    with torch.no_grad(): #不需要计算梯度\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size #梯度下降法更新参数\n",
    "            param.grad.zero_() #手动梯度归零"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some hyper-parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-2\n",
    "EPOCH = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "class RegDataSets(torch.utils.data.Dataset):\n",
    "    def __init__(self, true_w, true_b, sample_size):\n",
    "        self.features, self.labels = synthetic_data(true_w, true_b, sample_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "#data-loader\n",
    "reg_data_loader = torch.utils.data.DataLoader(RegDataSets(torch.tensor([2, -3.4]), 4.2, 1000), \n",
    "                                              batch_size=BATCH_SIZE, shuffle=True)\n",
    "# model \n",
    "net = torch.nn.Sequential(torch.nn.Linear(2, 1))\n",
    "# optimizer\n",
    "optim = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "# loss\n",
    "loss = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 826.020897\n",
      "epoch 2, loss 681.324340\n",
      "epoch 3, loss 534.184391\n",
      "epoch 4, loss 423.413263\n",
      "epoch 5, loss 338.355702\n",
      "epoch 6, loss 267.880443\n",
      "epoch 7, loss 211.172677\n",
      "epoch 8, loss 161.417068\n",
      "epoch 9, loss 124.391500\n",
      "epoch 10, loss 93.092874\n",
      "epoch 11, loss 70.407469\n",
      "epoch 12, loss 52.231450\n",
      "epoch 13, loss 37.931536\n",
      "epoch 14, loss 27.824126\n",
      "epoch 15, loss 20.043169\n"
     ]
    }
   ],
   "source": [
    "# train_loop with mean loss\n",
    "for epoch in range(EPOCH):\n",
    "    total_loss = 0\n",
    "    for X, y in reg_data_loader:\n",
    "        net.zero_grad()\n",
    "        l = loss(net(X), y)\n",
    "        total_loss += l.item()\n",
    "        l.backward()\n",
    "        optim.step()\n",
    "    print(f'epoch {epoch + 1}, loss {total_loss:f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.9855, -3.0008]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([3.6033], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in net[0].parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 32405.973846\n",
      "epoch 2, loss 27342.004089\n",
      "epoch 3, loss 23015.189224\n",
      "epoch 4, loss 19267.572769\n",
      "epoch 5, loss 16074.872162\n",
      "epoch 6, loss 13361.552704\n",
      "epoch 7, loss 11067.781307\n",
      "epoch 8, loss 9097.136734\n",
      "epoch 9, loss 7449.881424\n",
      "epoch 10, loss 6063.423054\n",
      "epoch 11, loss 4909.878922\n",
      "epoch 12, loss 3951.900068\n",
      "epoch 13, loss 3159.437819\n",
      "epoch 14, loss 2511.264574\n",
      "epoch 15, loss 1981.647661\n"
     ]
    }
   ],
   "source": [
    "# homework \n",
    "net = torch.nn.Sequential(torch.nn.Linear(2, 1))\n",
    "loss = torch.nn.MSELoss(reduction='sum')\n",
    "optim = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "# train_loop with sum loss\n",
    "for epoch in range(EPOCH):\n",
    "    total_loss = 0\n",
    "    for X, y in reg_data_loader:\n",
    "        net.zero_grad()\n",
    "        l = loss(net(X), y)\n",
    "        total_loss += l.item()\n",
    "        l.backward()\n",
    "        optim.step()\n",
    "    print(f'epoch {epoch + 1}, loss {total_loss:f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.3320, 15.8991]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# homework \n",
    "# 之后可以使用这一点进行高效的梯度裁剪\n",
    "net[0].weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.9769, -3.0305]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([2.9373], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in net[0].parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.3032, 0.5353]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2904], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 通过attribute访问一个网络模型的层和参数\n",
    "for layer in net.children():\n",
    "    if hasattr(layer, 'reset_parameters'):\n",
    "        layer.reset_parameters()\n",
    "for param in net[0].parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06f77016d31ee832f65100d09d66ee6db3390d0f686eae4accd2b20acd76aa64"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
